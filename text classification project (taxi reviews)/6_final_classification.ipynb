{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\olkos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\olkos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import *\n",
    "\n",
    "# Corpus Processing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize # tokenizing\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data cleaning & preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'an', 'am' 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n",
    "             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n",
    "             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n",
    "             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n",
    "             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']\n",
    "\n",
    "\n",
    "short_forms = {\n",
    "    \"don't\": \"do not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"must've\": \"must have\",\n",
    "    # Add more short forms and their full forms as needed\n",
    "}\n",
    "\n",
    "def replace_short_forms(text):\n",
    "    # Create a regular expression pattern to match short forms as standalone words\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(short_forms.keys()), re.IGNORECASE)\n",
    "    \n",
    "    # Replace short forms with their corresponding full forms using a lambda function\n",
    "    full_forms_text = re.sub(pattern, lambda match: short_forms[match.group(0)], text)\n",
    "    \n",
    "    return full_forms_text\n",
    "\n",
    "\n",
    "# (?) remove quotation marks, unnecessary punctuation, [{}[]\\/+*%|^%#@!?()]\n",
    "def punctuation_remover(text):\n",
    "    pattern = r'[{}\\[\\]\\\\\\/\\+\\*%\\|\\^%#@\\(\\)\\$\\\"]'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "# lemmatizing, tokenization, isalpha, stopwords\n",
    "def lemma_stopwords_token(text):\n",
    "      le=WordNetLemmatizer()\n",
    "      word_tokens=nltk.word_tokenize(text)\n",
    "      word_tokens =[token for token in word_tokens if token.isalpha()]\n",
    "      tokens=[le.lemmatize(token) for token in word_tokens if token not in stopwords and len(token)>2]\n",
    "      processed_text =\" \".join(tokens)\n",
    "      return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main preprocessing function\n",
    "def preprocessing(text):\n",
    "    reviews = replace_short_forms(text)\n",
    "    reviews = punctuation_remover(reviews)\n",
    "    reviews = lemma_stopwords_token(reviews)\n",
    "    return reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved sentiment model\n",
    "sentiment_model = joblib.load('model_sentiment_Naive_Bayes_SMOTE.joblib')\n",
    "\n",
    "# Load the saved topic classification model\n",
    "topic_classification_model = joblib.load('model_topic_classification.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset, preprocessing, vectorizer\n",
    "reviews = pd.read_csv(\"annotated_data_sentiment.csv\", encoding='utf-8')\n",
    "data = reviews['custom_comment'].apply(preprocessing)\n",
    "label = reviews['sentiment']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(text):\n",
    " \n",
    "  topics = ['1 Pricing and Fairness', '2 Driver professionalism', '3 Driver behaviour', '4 Customer Service', '5 Application', '6 Lost things', '7 Vehicle Condition', '8 Safety & reliability', '9 General bad', '10 Other']\n",
    "  predicted_result = []\n",
    "  \n",
    "  preprocessed_text = preprocessing(text)\n",
    "  features = vectorizer.transform([preprocessed_text])\n",
    "  prediction_sentiment = sentiment_model.predict(features)\n",
    "\n",
    "  if prediction_sentiment[0] == -1:\n",
    "    predicted_result.append(\"Sentiment: negative.\\nPredicted topic(s): \")\n",
    "    prediction = topic_classification_model.predict([preprocessed_text])\n",
    "    for i, topic in enumerate(topics):\n",
    "      if prediction[0][i] == 1:  \n",
    "        # predicted_result.append(topic) \n",
    "        predicted_result.append(''.join(topic))\n",
    "  else:\n",
    "    predicted_result.append(\"Sentiment: positive\")\n",
    "\n",
    "  return ' '.join(predicted_result) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. The driver didn't speak much and it was really good\n",
      "Sentiment: positive\n",
      "\n",
      "1. Nice driver, really helpful\n",
      "Sentiment: positive\n",
      "\n",
      "2. vehicle of premium class\n",
      "Sentiment: positive\n",
      "\n",
      "3. Great experience.I'd recommend this service to my friends.The driver started idle\n",
      "Sentiment: positive\n",
      "\n",
      "4. want refund\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  1 Pricing and Fairness\n",
      "\n",
      "5. driver didn't come to our location and switched on idle\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  1 Pricing and Fairness 2 Driver professionalism\n",
      "\n",
      "6. The driver swore and racist\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  3 Driver behaviour\n",
      "\n",
      "7. I had a couple of rides with your service before and they were nice, but this time there wasn't a seatbelt which I believe is totally not OK. and the driver just said Are yoou going or not??\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  3 Driver behaviour\n",
      "\n",
      "8. left  my phone in the car!\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  6 Lost things\n",
      "\n",
      "9. I lost my wallet! and your customer support never answered my message\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  4 Customer Service\n",
      "\n",
      "10. your customer service never replies\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  4 Customer Service\n",
      "\n",
      "11. your app doesn't let me choose 2 locations\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  2 Driver professionalism\n",
      "\n",
      "12. driver is a cheat. I lost my laptop!\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  3 Driver behaviour\n",
      "\n",
      "13. bad smell in the car. dirty\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  7 Vehicle Condition\n",
      "\n",
      "14. bad\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  9 General bad\n",
      "\n",
      "15. it was cold in the car\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  7 Vehicle Condition\n",
      "\n",
      "16. too fast driving style.\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  2 Driver professionalism 8 Safety & reliability\n",
      "\n",
      "17. no seat belt at the back seat.\n",
      "Sentiment: negative.\n",
      "Predicted topic(s):  7 Vehicle Condition 8 Safety & reliability\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_example = [\"\"\"The driver didn't speak much and it was really good\"\"\",\n",
    "                   \"\"\"Nice driver, really helpful\"\"\",\n",
    "                \"\"\"vehicle of premium class\"\"\",\n",
    "                \"\"\"Great experience.I'd recommend this service to my friends.\"\"\"\n",
    "\n",
    "                \"\"\"The driver started idle\"\"\",\n",
    "                \"\"\"want refund\"\"\",\n",
    "                \"\"\"driver didn't come to our location and switched on idle\"\"\",\n",
    "                \"\"\"The driver swore and racist\"\"\",\n",
    "                \"\"\"I had a couple of rides with your service before and they were nice, but this time there wasn't a seatbelt which I believe is totally not OK. and the driver just said Are yoou going or not??\"\"\",\n",
    "                \"\"\"left  my phone in the car!\"\"\",\n",
    "                \"\"\"I lost my wallet! and your customer support never answered my message\"\"\",\n",
    "                \"\"\"your customer service never replies\"\"\",\n",
    "                \"\"\"your app doesn't let me choose 2 locations\"\"\",\n",
    "                \"\"\"driver is a cheat. I lost my laptop!\"\"\",\n",
    "                \"\"\"bad smell in the car. dirty\"\"\",\n",
    "                \"\"\"bad\"\"\",\n",
    "                \"\"\"it was cold in the car\"\"\",\n",
    "                \"\"\"too fast driving style.\"\"\",\n",
    "                \"\"\"no seat belt at the back seat.\"\"\"        \n",
    "                ]\n",
    "\n",
    "for i, review in enumerate(reviews_example):\n",
    "  # print(f\"{review[0]}. {prepare_new_reviews(review[1])}\")\n",
    "  print(f\"{i}. {review}\")\n",
    "  print(classification(review))\n",
    "  print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification method for gradio visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getClassification(text):\n",
    "  sentiment_model = joblib.load('model_sentiment_Naive_Bayes.joblib')\n",
    "  topic_classification_model = joblib.load('model_topic_classification.joblib')\n",
    "  topics = ['1 Pricing and Fairness', '2 Driver professionalism', '3 Driver behaviour', '4 Customer Service', '5 Application', '6 Lost things', '7 Vehicle Condition', '8 Safety & reliability', '9 General bad', '10 Other']\n",
    "  \n",
    "\n",
    "  predicted_topics = []\n",
    "  preprocessed_text = preprocessing(text)\n",
    "  prediction_sentiment = sentiment_model.predict([preprocessed_text])\n",
    "  result = []\n",
    "  if prediction_sentiment[0] == -1:\n",
    "    result.append(\"negative\")\n",
    "    prediction = topic_classification_model.predict([preprocessed_text])\n",
    "    for i, topic in enumerate(topics):\n",
    "      if prediction[0][i] == 1:\n",
    "        predicted_topics.append(topic)\n",
    "    result.extend(predicted_topics)\n",
    "  else :\n",
    "    result.append(\"positive\")\n",
    "  \n",
    "  return \", \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\olkos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a79a2b281fc2a70025.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a79a2b281fc2a70025.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=getClassification, \n",
    "    inputs=gr.Textbox(placeholder=\"Enter a review here...\"), \n",
    "    outputs=[\"label\"], \n",
    "    interpretation=\"default\",\n",
    "    examples=[[\"It was wonderful!\"]])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
